# AI Governance in Specula Method

## Overview

Artificial intelligence is present throughout the Specula Method, but always under clear governance. AI never "decides"—people define problems, constraints, radical values, and acceptance criteria. AI's role is as an amplifier of complexity and maieutic partner.

This document outlines how AI is used responsibly within the framework, what limitations we acknowledge, and what principles guide its deployment.

---

## Four Types of AI Usage

### 1. Signal Analysis
**Purpose**: Reading large volumes of content to surface patterns, outliers, contradictions, and weak signals.

**How it works**:
- NLP models analyze trend reports, articles, social media, papers, stakeholder conversations
- Pattern recognition identifies recurring themes and anomalies
- Outlier detection surfaces perspectives that might be ignored
- Correlation mapping finds unexpected connections between disparate trends

**Human role**: Interpreting significance, validating patterns, deciding which signals matter

**Example**: Analyzing 500 sustainability reports to identify emerging language shifts around "regenerative" vs "sustainable" practices.

---

### 2. Scenario Generation
**Purpose**: Building plausible and preferable future scenarios by combining multiple drivers.

**How it works**:
- Language and visual models generate scenarios combining cultural, technological, economic, regulatory, and environmental factors
- High-temperature configurations (more randomness) encourage unexpected combinations
- Models explicitly prompted to generate "uncomfortable" scenarios where brand assumptions break down

**Human role**: Selecting which scenarios to develop further, ensuring plausibility, choosing preferable directions

**Example**: Generating a 2030 scenario where water scarcity makes traditional cosmetics manufacturing impossible, forcing brands to reimagine production entirely.

---

### 3. Conversational Facilitation
**Purpose**: Acting as Socratic sparring partner to help teams surface assumptions, fears, desires, and conflicts.

**How it works**:
- Conversational agents designed to ask questions, not provide answers
- Prompts configured to challenge assumptions and surface contradictions
- Designed to be uncomfortable—pushing teams beyond comfort zones
- No directive guidance, only maieutic questioning

**Human role**: Doing the actual thinking, making decisions, owning conclusions

**Example**: AI asks "If this value is truly non-negotiable, what profitable opportunity have you refused because of it?" forcing teams to examine authenticity of stated values.

---

### 4. Synthesis and Alignment
**Purpose**: Transforming raw workshop materials into coherent narratives and structured documents.

**How it works**:
- Clustering and categorization of post-its, transcripts, research outputs
- Narrative synthesis maintaining team voice and intent
- Gap analysis between intended direction and actual content
- Template population and guideline drafting

**Human role**: Reviewing, editing, approving all synthesized content; ensuring fidelity to original intent

**Example**: Converting 4 hours of workshop discussion into structured Speculative Identity Canvas with direct quotes and synthesized themes.

---

## Non-Negotiable Principles

### 1. Human-in-the-Loop (Mandatory)
No strategic decision can be delegated to AI. Every output is discussed in a human setting with explicit responsibilities.

**What this means in practice**:
- AI can suggest, never decide
- Minimum two humans review every AI-generated insight
- Strategic choices (positioning, values, products, policies) require human consensus
- AI outputs are inputs to human decision-making, not decisions themselves

### 2. Double Validation
AI-generated insights are reviewed by at least two senior figures with different backgrounds (strategy, research, ethics, design).

**Why different backgrounds matter**:
- Strategy perspective catches business misalignment
- Research perspective validates factual accuracy
- Ethics perspective surfaces values violations
- Design perspective ensures practical feasibility

### 3. Bias Audit
At project start, explicit discussion of AI data quality and provenance. During work, active monitoring for distorting patterns.

**Common biases to watch for**:
- Exclusion of non-Western perspectives or Global South contexts
- Stereotyping based on demographic data in training sets
- Normalization of extractive capitalist practices
- Optimization for engagement over wellbeing
- Privileging of quantifiable over qualitative insights

**Audit process**:
- Document what data AI is trained on
- Identify known biases in those datasets
- Flag outputs that show bias patterns
- Explicitly correct or reject biased outputs

### 4. Transparency with Client
Always clear when content is AI-generated, co-generated, or synthesized. Technology use is never hidden.

**Documentation standards**:
- AI-generated content tagged as such
- Co-created content shows human and AI contributions
- Synthesis processes explained in deliverables
- Prompt strategies shared when relevant
- No "AI washing" of human work or vice versa

---

## Acknowledged Limitations

We explicitly recognize that AI:

### 1. Replicates Historical Biases
AI trained on historical data reproduces the biases, exclusions, and power structures in that data. It doesn't imagine truly different futures—it extrapolates from existing patterns.

**Our response**: Use AI to surface possibilities, then apply critical filters. Explicitly seek perspectives AI might systematically miss.

### 2. Misses Cultural Nuance
AI struggles with embodied experience, local cultural contexts, and subtleties that aren't well-represented in training data.

**Our response**: Combine AI outputs with ethnographic research, local expertise, and community dialogue. Never rely on AI alone for cultural interpretation.

### 3. Confuses Plausible with Preferable
AI generates coherent scenarios efficiently, but has no intrinsic understanding of what's ethically desirable. It can make harmful futures sound compelling.

**Our response**: The Ethical Gate exists precisely because AI cannot make these judgments. Every prototype is human-reviewed against radical values.

### 4. Lacks Value Understanding
AI doesn't "understand" declared values except through constraints we design into prompts and evaluation criteria.

**Our response**: Values are defined by humans in Phase 2 and encoded as explicit filters. AI operates within those constraints, not independently.

---

## Governance in Each Phase

### Phase 1: Scenario Generation
**AI Role**: High creativity, exploratory  
**Governance**: Human selection of which scenarios advance, validation of plausibility  
**Temperature**: High (0.8-0.9) to encourage divergence  
**Review**: All scenarios reviewed for bias and plausibility before client presentation

### Phase 2: Brand Archeology
**AI Role**: Pattern recognition in existing materials  
**Governance**: Humans define radical values, AI surfaces tensions  
**Temperature**: Medium (0.6-0.7)  
**Review**: Synthesis validated against actual organizational behavior and history

### Phase 3: Future Prototyping
**AI Role**: Prototype generation and stress-testing  
**Governance**: Ethical Gate with human reviewers, mandatory human approval  
**Temperature**: Medium-high (0.7-0.8)  
**Review**: Every prototype through Ethical Gate before advancing

### Phase 4: Narrative Synthesis
**AI Role**: Draft narratives, explore registers and voices  
**Governance**: Human selection of tone, approval of final narratives  
**Temperature**: Medium (0.5-0.7)  
**Review**: Client team approves all narrative directions and final copy

### Phase 5: Co-creation
**AI Role**: Aggregate community input, identify patterns  
**Governance**: Humans interpret significance, make final decisions on incorporation  
**Temperature**: Low-medium (0.4-0.6) for faithful synthesis  
**Review**: Community input summarized but not filtered by AI alone

### Phase 6: Activation & Guardian
**AI Role**: Signal monitoring, drift detection  
**Governance**: Humans interpret signals and decide on response  
**Temperature**: Low (0.3-0.5) for accurate monitoring  
**Review**: Quarterly human review of all AI-flagged signals

---

## Configuration Transparency

While specific prompt libraries and configurations remain proprietary, we commit to:

**What we share**:
- General approach to temperature settings
- Types of constraints applied
- Bias mitigation strategies
- Human review checkpoints

**What remains proprietary**:
- Exact prompt formulations
- Scenario libraries and databases
- Custom fine-tuning approaches
- Client-specific configurations

---

## When AI Should NOT Be Used

AI is inappropriate for:

### 1. Final Strategic Decisions
AI can inform but never make decisions about brand positioning, values hierarchy, or strategic direction.

### 2. Ethical Judgment
AI cannot determine what's ethically acceptable. The Ethical Gate requires human judgment.

### 3. Community Representation
AI cannot speak for communities or stakeholders. Human dialogue is required.

### 4. Crisis Response
During crises, human judgment and empathy are essential. AI can support but not lead.

### 5. Value Trade-offs
When values conflict, humans must decide priorities. AI can surface the conflict but not resolve it.

---

## Evolving Governance

This governance framework evolves as:
- AI capabilities change
- New use cases emerge
- Failures reveal gaps
- Community provides feedback

**How to propose changes**:
1. Open an issue describing the limitation or risk
2. Provide specific examples from practice
3. Suggest concrete governance improvements
4. Engage in community discussion

---

## Integration with Ethical Framework

AI Governance works in tandem with the [Ethical Framework](./ethical-framework.md):

- **Ethical Framework** defines what values and boundaries must be respected
- **AI Governance** defines how AI is used within those boundaries
- Together they ensure technology serves values, not the reverse

---

## Questions & Discussion

- How do you ensure AI doesn't subtly steer outcomes toward extractive patterns?
- What happens when AI-generated scenarios are more compelling than human judgment says they should be?
- How do you handle situations where AI surfaces truths organizations don't want to see?

Join the conversation in [GitHub Discussions](https://github.com/oddtitoreal/specula-method/discussions).

---

*This governance framework is part of Specula Method v1.0 open documentation. It will evolve based on practice and community input.*

*Last updated: January 2026*
